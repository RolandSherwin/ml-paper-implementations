{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90631847",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "Can you _keep learning better networks by just stacking more layers?_ An obstacle to answering that question was the **Unstable Gradient Problem** which was solved by using various _weight initialization techniques_ and _batch-normalization_. Now as we are able to build deeper networks that actually converge, we face a new problem. With the network depth increasing, accuracy gets saturated and then  **degrades rapidly.**\n",
    "\n",
    "This can be explained by the following.Consider we add a new layer to a shallow network, the we assume that the new layer should learn atleast the **identity mapping, i.e., give it should atleast produce its input as output.** But they don't actually do so.\n",
    "\n",
    "### Residual Block (or) Skip Connections\n",
    "\n",
    "We can overcome this by using Residual blocks, which takes the activations(x) from layer l and passing it to the activation function-g in layer l+2; i.e., a[l+2] = g(W[l+2]a[l+1] + a[l]) = g(Z[l+2] + a[l]) \n",
    "\n",
    "Thus the sahpe of a[l] and Z[l+2] should be the same. If they are different we can multiply the activation with a learnable matrix Ws; i.e., a[l+2] = g(z[l+2] + Ws*a[l])\n",
    "![](../images/resNet.png)\n",
    "\n",
    "This works because: Assume you don't learn anything in the new l+2, and consider using stong regularization; then z[l+2] is close to 0. But since we passed in the activation a[l], we get a[l+2] = g(0 + a[l]) = a[l] (assuming we use ReLU which just returns a[l] as output since a[l]>0). Thus our new layer - l+2 just acts a identity function even in worst case scenario. Thus if we are just able to learn anything new, our performance always increases and never decreases.\n",
    "\n",
    "What goes wrong in the non-residual block network is that it **fails to even learn the identity function,** thus having a lot of layers will hurt the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ccebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ml': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0597ef2c4e330346690cfc3eee48a64f4a5787db7dc7ff65ed373f1569f4e13ec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
